{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d670906",
   "metadata": {},
   "source": [
    "# Title: **Write a Program to Implement Reinforcement Learning in a Grid World**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4b510",
   "metadata": {},
   "source": [
    "## Objective: To develop a Python program that demonstrates reinforcement learning through the Q-learning algorithm in a simple grid world setting, illustrating basic concepts such as exploration vs. exploitation and learning optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb43fb",
   "metadata": {},
   "source": [
    "### Theory\n",
    "Reinforcement Learning (RL) is an area of machine learning where an agent learns to behave in an environment by performing actions and receiving rewards. It uses these experiences to learn a strategy or policy that maximizes the cumulative reward it receives over time. \n",
    "\n",
    "**Q-learning** is a model-free off-policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It does not require a model of the environment and can handle problems with stochastic transitions and rewards without requiring adaptations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809aed2",
   "metadata": {},
   "source": [
    "### Materials/Tools Required\n",
    "- Python 3.x installed on a computer\n",
    "- Python libraries: `numpy`\n",
    "- Text editor or Integrated Development Environment (IDE) such as PyCharm, Visual Studio Code, or Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f54657",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "1. Install the required Python library using pip:\n",
    "   ```bash\n",
    "   pip install numpy\n",
    "   ```\n",
    "2. Open your Python development environment.\n",
    "3. Type the provided code into the editor.\n",
    "4. Save the file with a `.py` extension, for example, `grid_world_q_learning.py`.\n",
    "5. Run the program to perform Q-learning and observe the learning of the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6768f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right right right down down \n",
      "up wall right right down \n",
      "right down wall right down \n",
      "right down down wall down \n",
      "right right right right up \n"
     ]
    }
   ],
   "source": [
    "### Python Program Code\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, width, height, start, terminal_states, obstacles, reward_size):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.position = start\n",
    "        self.terminal_states = terminal_states\n",
    "        self.obstacles = obstacles\n",
    "        self.reward_size = reward_size\n",
    "        self.state_space = [(x, y) for x in range(width) for y in range(height) if (x, y) not in obstacles]\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        self.q_table = dict((state, dict((action, 0.0) for action in self.action_space)) for state in self.state_space)\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        x, y = state\n",
    "        if action == 'up':\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 'down':\n",
    "            y = min(y + 1, self.height - 1)\n",
    "        elif action == 'left':\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 'right':\n",
    "            x = min(x + 1, self.width - 1)\n",
    "        \n",
    "        next_state = (x, y)\n",
    "        return next_state if next_state not in self.obstacles else state\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        return self.reward_size if state in self.terminal_states else -1\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.get_next_state(self.position, action)\n",
    "        reward = self.get_reward(next_state)\n",
    "        self.position = next_state\n",
    "        return next_state, reward, self.is_terminal_state(next_state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = (0, 0)  # Reset to the start position\n",
    "        return self.position\n",
    "\n",
    "def train_agent(grid_world, episodes, alpha, gamma, epsilon):\n",
    "    for episode in range(episodes):\n",
    "        state = grid_world.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(grid_world.action_space)\n",
    "            else:\n",
    "                action = max(grid_world.q_table[state], key=grid_world.q_table[state].get)\n",
    "            \n",
    "            next_state, reward, done = grid_world.step(action)\n",
    "            old_value = grid_world.q_table[state][action]\n",
    "            future_optimal_value = max(grid_world.q_table[next_state].values())\n",
    "            new_value = old_value + alpha * (reward + gamma * future_optimal_value - old_value)\n",
    "            grid_world.q_table[state][action] = new_value\n",
    "            state = next_state\n",
    "\n",
    "def print_policy(grid_world):\n",
    "    for y in range(grid_world.height):\n",
    "        for x in range(grid_world.width):\n",
    "            state = (x, y)\n",
    "            if state in grid_world.state_space:\n",
    "                print(max(grid_world.q_table[state], key=grid_world.q_table[state].get), end=' ')\n",
    "            else:\n",
    "                print('wall', end=' ')\n",
    "        print()\n",
    "\n",
    "# Define grid world environment\n",
    "width, height = 5, 5\n",
    "start = (0, 0)\n",
    "terminal_states = [(4, 4)]\n",
    "obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "reward_size = 100\n",
    "\n",
    "# Create grid world\n",
    "env = GridWorld(width, height, start, terminal_states, obstacles, reward_size)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, 1000, 0.01, 0.99, 0.1)\n",
    "\n",
    "# Output the learned policy\n",
    "print_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d9a84",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Observe the convergence of the Q-learning algorithm as it learns to navigate the grid to reach the terminal state while avoiding obstacles.\n",
    "- Note the exploration (choosing random actions) and exploitation (choosing the best-known action) balance maintained through the epsilon parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2702e3f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The Q-learning algorithm successfully learns an optimal policy for navigating a grid world environment, demonstrating fundamental concepts of reinforcement learning such as Q-values, rewards, and exploration-exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494728a",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Autonomous navigation systems\n",
    "- Game AI\n",
    "- Decision-making systems in uncertain environments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
